{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WculWPI2Cwd8"
   },
   "source": [
    "# Lab 04 : Diffusion Model (DDPM) for MNIST Images -- exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5242_2025_codes/labs_lecture08/lab04_dm_image'\n",
    "    print(path_to_file)\n",
    "    # move to Google Drive directory\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPWi0LPuCweC",
    "outputId": "66a77b7d-df7d-4f20-9cf9-5ed329d213ce"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "#import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL) # remove warnings\n",
    "import os, datetime\n",
    "\n",
    "# PyTorch version and GPU\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device= torch.device(\"cuda\") # use GPU\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_mnist_dataset_exists\n",
    "data_path=check_mnist_dataset_exists()\n",
    "\n",
    "train_data=torch.load(data_path+'mnist/train_data.pt')\n",
    "print(train_data.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uMguCwhECweH",
    "outputId": "5d4eabb0-6756-48a2-c57c-be281774cdf9"
   },
   "outputs": [],
   "source": [
    "# Global constants\n",
    "n = train_data.size(1) # num of pixels along one spatial dimension\n",
    "dz = 128 # latent dimension\n",
    "dID = 128 # hidden dimension for ID features\n",
    "bs = 100 # batch size\n",
    "N = train_data.size(0) # num of training data\n",
    "print('n,dz,dID,bs,N:',n,dz,dID,bs,N)\n",
    "\n",
    "d = 64 # hidden dimension for image features\n",
    "d = 48\n",
    "dPE = 128 # hidden dimension for time \n",
    "beta_1 = 0.0001\n",
    "beta_T = 0.02\n",
    "num_t = 150\n",
    "print('beta_1,beta_T,num_t,d,dPE:',beta_1,beta_T,num_t,d,dPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPM denoiser with UNet architecture\n",
    "https://arxiv.org/pdf/1505.04597\n",
    "\n",
    "Diffusion models require an expressive denoiser to predict the noise that is added to the clean image. A standard denoiser for image is UNet.\n",
    "\n",
    "The task is to implement UNet, which is designed according to the diagram below:\n",
    "<center>\n",
    "<img src=\"pic/unet.png\" style=\"height:500px\"/>\n",
    "</center>\n",
    "    \n",
    "Implement UNet with batch normalization, ReLU activation and residual connection.\n",
    "\n",
    "Hints: You may use PyTorch modules `nn.Conv2d`, `nn.ConvTranspose2d` and `nn.BatchNorm2d`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWg-3b7bCweH",
    "outputId": "ccdd54e5-8aea-4cbb-f67c-ab10609f94a6"
   },
   "outputs": [],
   "source": [
    "# Network design\n",
    "\n",
    "class first_block(nn.Module): \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # COMPLETE HERE\n",
    "        \n",
    "    def forward(self, h, t):\n",
    "        # Add time information t to h\n",
    "        t = # COMPLETE HERE # [bs, in_dim]\n",
    "        h = # COMPLETE HERE               # [bs, in_dim, in_n, in_n]\n",
    "        # First convolution layer\n",
    "        h = # COMPLETE HERE           # [bs, out_dim, in_n, in_n]\n",
    "        # Second convolution layer\n",
    "        h = # COMPLETE HERE       # [bs, out_dim, in_n, in_n]\n",
    "        return h\n",
    "\n",
    "class down_sampling_block(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, padding):\n",
    "        super().__init__()\n",
    "        # COMPLETE HERE\n",
    "        \n",
    "    def forward(self, h, t):\n",
    "        # Add time information t to h\n",
    "        t = # COMPLETE HERE # [bs, in_dim]\n",
    "        h = # COMPLETE HERE               # [bs, in_dim, in_n, in_n]        \n",
    "        # First convolution layer\n",
    "        h = # COMPLETE HERE       # [bs, out_dim, in_n/2, in_n/2]\n",
    "        # Second convolution layer\n",
    "        h = # COMPLETE HERE       # [bs, out_dim, in_n/2, in_n/2]\n",
    "        # Third convolution layer\n",
    "        h = # COMPLETE HERE       # [bs, out_dim, in_n/2, in_n/2]\n",
    "        return h\n",
    "\n",
    "class up_sampling_block(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, output_padding):\n",
    "        super().__init__()\n",
    "        # COMPLETE HERE\n",
    "        \n",
    "    def forward(self, h_level, h_level_minus_one, t):\n",
    "        # Add time information t to h\n",
    "        t = # COMPLETE HERE      # [bs, in_dim=2*out_dim]\n",
    "        # First convolution layer\n",
    "        h_level = # COMPLETE HERE  # [bs, out_dim, in_n*2, in_n*2]\n",
    "        # Concatenate down-sampling and up-sampling\n",
    "        h = # COMPLETE HERE # [bs, in_dim=2*out_dim, in_n*2, in_n*2]\n",
    "        # Second convolution layer\n",
    "        h = # COMPLETE HERE                # [bs, out_dim, in_n*2, in_n*2]\n",
    "        # Third convolution layer\n",
    "        h = # COMPLETE HERE            # [bs, out_dim, in_n*2, in_n*2]           \n",
    "        return h\n",
    "\n",
    "# Define UNet architecture\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # COMPLETE HERE\n",
    "    \n",
    "    def forward(self, h_t, sample_t):\n",
    "        h = # COMPLETE HERE                        # [bs, 1, n, n], [100, 1, 28, 28]\n",
    "        t = # COMPLETE HERE                 # [bs, 1], [100, 1]\n",
    "        h1_down = # COMPLETE HERE            # [bs, d, n, n], [100, 64, 28, 28]\n",
    "        h2_down = # COMPLETE HERE      # [bs, 2d, n/2, n/2], [100, 128, 14, 14]     \n",
    "        h3_down = # COMPLETE HERE      # [bs, 4d, n/4, n/4], [100, 256, 7, 7]\n",
    "        h4_down = # COMPLETE HERE      # [bs, 8d, n/8, n/8], [100, 512, 4, 4]\n",
    "        h3_up = # COMPLETE HERE # [bs, 4d, n/4, n/4], [100, 256, 7, 7]\n",
    "        h2_up = # COMPLETE HERE   # [bs, 2d, n/2, n/2], [100, 128, 14, 14]\n",
    "        h1_up = # COMPLETE HERE   # [bs, d, n, n], [100, 64, 28, 28] \n",
    "        h = # COMPLETE HERE                   # [bs, 1, n, n], [bs, 1, 28, 28]\n",
    "        h = # COMPLETE HERE                             # [bs, n, n], [bs, 28, 28]\n",
    "        return h\n",
    "\n",
    "# Define DDPM architecture\n",
    "class DDPM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_t, beta_1, beta_T):\n",
    "        super().__init__()\n",
    "        self.num_t = num_t\n",
    "        self.alpha_t = 1.0 - torch.linspace(beta_1, beta_T, num_t).to(device) # [num_t]\n",
    "        self.alpha_bar_t = torch.cumprod( self.alpha_t, dim=0) # [num_t]\n",
    "        self.UNet = UNet()\n",
    "\n",
    "    def forward_process(self, x0, sample_t, eps): # add noise\n",
    "        sqrt_alpha_bar_t = self.alpha_bar_t[sample_t].sqrt() # [bs]\n",
    "        sqrt_one_minus_alpha_bar_t = ( 1.0 - self.alpha_bar_t[sample_t] ).sqrt() # [bs]\n",
    "        x_t = sqrt_alpha_bar_t.view(bs,1,1) * x0 + sqrt_one_minus_alpha_bar_t.view(bs,1,1) * eps # [bs, n, n]\n",
    "        return x_t\n",
    "\n",
    "    def backward_process(self, x_t, sample_t): # denoise\n",
    "        x_t_minus_one = self.UNet(x_t, sample_t) # [bs, n, n]\n",
    "        return x_t_minus_one\n",
    "\n",
    "    def generate_process_ppdm(self, num_images):\n",
    "        t = num_t-1\n",
    "        batch_t = (t * torch.ones(num_images)).long().to(device)\n",
    "        batch_x_t = torch.randn(num_images, n, n).to(device) # t=T => t=T-1 in python\n",
    "        set_t = list(range(t-1,0,-1)); set_t = set_t + [0]\n",
    "        # print('num_steps:',len(set_t)+1,'set_t:',set_t)\n",
    "        for t_minus_one in set_t: # for t=T,T-step_size,T-2*step_size,...,step_size,0\n",
    "            batch_t_minus_one = (t_minus_one * torch.ones(num_images)).long().to(device)\n",
    "            batch_noise_pred_t = self.backward_process(batch_x_t, batch_t)\n",
    "            sigma_t = ( (1.0-self.alpha_bar_t[t_minus_one])/ (1.0-self.alpha_bar_t[t])* (1.0-self.alpha_bar_t[t]/self.alpha_bar_t[t_minus_one]) ).sqrt()\n",
    "            c1 = self.alpha_bar_t[t_minus_one].sqrt() / self.alpha_bar_t[t].sqrt()\n",
    "            c2 = ( 1.0 - self.alpha_bar_t[t] + 1e-10 ).sqrt()\n",
    "            c3 = ( 1.0 - self.alpha_bar_t[t_minus_one] - sigma_t.square() + 1e-10 ).sqrt()\n",
    "            batch_x_t_minus_one = c1 * ( batch_x_t - c2 * batch_noise_pred_t ) + c3 * batch_noise_pred_t + sigma_t* torch.randn(num_images, n, n).to(device)\n",
    "            t = t_minus_one\n",
    "            batch_x_t = batch_x_t_minus_one\n",
    "            batch_t = batch_t_minus_one\n",
    "        return batch_x_t\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the network\n",
    "net = DDPM(num_t, beta_1, beta_T)\n",
    "net = net.to(device)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "display_num_param(net)\n",
    "\n",
    "\n",
    "# Test the forward pass, backward pass and gradient update with a single batch\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True)\n",
    "idx_images = torch.LongTensor(bs).random_(0,N)\n",
    "batch_x0 = train_data[idx_images,:,:].to(device) # [bs, n, n]\n",
    "batch_sample_t = torch.randint(0, num_t, (bs,)).long().to(device) # random interger in {0,1,...,T-1} [bs]\n",
    "print('batch_sample_t',batch_sample_t.size())\n",
    "batch_noise_t = torch.randn(batch_x0.size()).to(device) # [bs, n, n]\n",
    "x_t = net.forward_process(batch_x0, batch_sample_t, batch_noise_t) # [bs, n, n]\n",
    "print('x_t',x_t.size())\n",
    "noise_pred_t = net.backward_process(x_t, batch_sample_t) # [bs, n, n]\n",
    "print('noise_pred_t',noise_pred_t.size())\n",
    "loss_PPDM = torch.nn.MSELoss()(noise_pred_t, batch_noise_t)\n",
    "loss = loss_PPDM\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "with torch.no_grad():\n",
    "    batch_x_0 = net.generate_process_ppdm(4)\n",
    "    print('batch_x_0',batch_x_0.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77Mdin1WCweJ",
    "outputId": "cc85e172-36e4-4f58-d01b-b96bc4e34437",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Training loop\n",
    "net = DDPM(num_t, beta_1, beta_T)\n",
    "net = net.to(device)\n",
    "display_num_param(net)\n",
    "\n",
    "# Optimizer\n",
    "init_lr = 0.0003\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=init_lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True)\n",
    "\n",
    "# Number of mini-batches per epoch\n",
    "nb_epochs = 20\n",
    "\n",
    "# Training loop\n",
    "start = time.time()\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    shuffled_indices = torch.randperm(60000)\n",
    "\n",
    "    for count in range(0,60000,bs):\n",
    "\n",
    "        idx_images = shuffled_indices[count : count+bs]\n",
    "        batch_x0 = train_data[idx_images,:,:].to(device) # [bs, n, n]\n",
    "        batch_sample_t = torch.randint(0, num_t, (bs,)).long().to(device) # [bs]\n",
    "        batch_noise_t = torch.randn(batch_x0.size()).to(device) # [bs, n, n]\n",
    "        x_t = net.forward_process(batch_x0, batch_sample_t, batch_noise_t) # [bs, n, n]\n",
    "        noise_pred_t = net.backward_process(x_t, batch_sample_t) # [bs, n, n]\n",
    "        loss_PPDM = torch.nn.MSELoss()(noise_pred_t, batch_noise_t)\n",
    "        loss = loss_PPDM\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # COMPUTE STATS\n",
    "        running_loss += loss.detach().item()\n",
    "        num_batches += 1\n",
    "\n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    total_loss = running_loss/num_batches\n",
    "    scheduler.step(total_loss)\n",
    "    elapsed = (time.time()-start)/60\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'min', '\\t lr=', optimizer.param_groups[0]['lr']  ,'\\t loss=', total_loss )\n",
    "\n",
    "    # PLOT\n",
    "    if epoch>0 and not epoch%5:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            num_generated_images = 16\n",
    "            batch_x_0 = net.generate_process_ppdm(num_generated_images)\n",
    "            x_hat = batch_x_0.squeeze().detach().to('cpu')\n",
    "        figure, axis = plt.subplots(4, 4)\n",
    "        figure.set_size_inches(10,10)\n",
    "        i,j,cpt=0,0,0; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=1,0,1; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=2,0,2; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=3,0,3; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=0,1+0,4; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=1,1+0,5; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=2,1+0,6; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=3,1+0,7; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=0,2+0,8; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=1,2+0,9; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=2,2+0,10; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=3,2+0,11; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=0,3+0,12; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=1,3+0,13; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=2,3+0,14; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        i,j,cpt=3,3+0,15; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "        plt.show()\n",
    "        net.train()\n",
    "\n",
    "    # Check lr value\n",
    "    if optimizer.param_groups[0]['lr'] < 2*10**-4: \n",
    "        print(\"\\n lr is equal to min lr -- training stopped\\n\")\n",
    "        break\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "id": "V7nYE_UHHl3_",
    "outputId": "8fecead3-7750-47f2-e303-e9d023e7a548"
   },
   "outputs": [],
   "source": [
    "# Generated images with DDPM\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    num_generated_images = 16\n",
    "    batch_x_0 = net.generate_process_ppdm(num_generated_images)\n",
    "    print('batch_x_0',batch_x_0.size())\n",
    "    x_hat = batch_x_0.squeeze().detach().to('cpu')\n",
    "\n",
    "figure, axis = plt.subplots(4, 4)\n",
    "figure.set_size_inches(10,10)\n",
    "\n",
    "i,j,cpt=0,0,0; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=1,0,1; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=2,0,2; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=3,0,3; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=0,1+0,4; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=1,1+0,5; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=2,1+0,6; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=3,1+0,7; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=0,2+0,8; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=1,2+0,9; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=2,2+0,10; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=3,2+0,11; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=0,3+0,12; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=1,3+0,13; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=2,3+0,14; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "i,j,cpt=3,3+0,15; axis[i,j].imshow(x_hat[cpt,:,:], cmap='gray'); axis[i,j].set_title(\"Generated w/ DDPM\"); axis[i,j].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP1NER3B8soj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
