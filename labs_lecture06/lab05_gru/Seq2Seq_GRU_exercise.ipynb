{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBpDhW46p-PT"
   },
   "source": [
    "# Lab03: Custom GRU for Text Generation\n",
    "\n",
    "In this lab, you will build a sequence-to-sequence model for text generation.  \n",
    "\n",
    "# Introduction to Sequence-to-Sequence (Seq2Seq) Models\n",
    "\n",
    "Sequence-to-Sequence (Seq2Seq) models are a class of neural networks designed to map an input sequence to an output sequence, even when the two sequences have different lengths. They are widely used in applications such as machine translation, text summarization, dialogue systems, and text generation.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The **encoder** processes the entire input sequence (for example, a sentence or a series of tokens) one token at a time. Typically implemented using recurrent neural networks (RNNs) such as GRU or LSTM, the encoder compresses the input into a fixed-dimensional vector, often referred to as the **context vector** or **hidden state**. This vector is intended to capture the semantic and syntactic information of the input sequence.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The **decoder** takes the context vector produced by the encoder and generates the output sequence one token at a time. Also implemented as an RNN, the decoder uses the context vector to initialize its hidden state and then generates an output token. It uses its previous output (or a combination of that output and the context) to predict the next token, repeating this process until an end-of-sequence token is produced.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Encoding the Input:**\n",
    "   - The encoder processes the input sequence \\( \\{x_1, x_2, \\ldots, x_T\\} \\) and updates its hidden state at each time step.\n",
    "   - The final hidden state \\( h_T \\) (or a combination of hidden states) is used as a summary of the input.\n",
    "\n",
    "2. **Decoding the Output:**\n",
    "   - The decoder initializes its hidden state using the context vector \\( h_T \\).\n",
    "   - It then generates the output sequence \\( \\{y_1, y_2, \\ldots, y_{T'}\\} \\) by predicting one token at a time.\n",
    "   - At each decoding step, the decoder uses its current hidden state and possibly the previous token to predict the next token.\n",
    "\n",
    "3. **Handling Variable Lengths:**\n",
    "   - Because both the encoder and decoder are recurrent, they can process sequences of variable lengths. This flexibility makes Seq2Seq models effective for many different tasks.\n",
    "\n",
    "\n",
    "## Task\n",
    "\n",
    "You will train the model on a sample text  \n",
    "corpus and generate text from the trained model.\n",
    "\n",
    "The GRU cell is defined with the following equations:\n",
    "\n",
    "  rₜ = σ(Wᵣ xₜ + Uᵣ hₜ₋₁)  \n",
    "  zₜ = σ(W_z xₜ + U_z hₜ₋₁)  \n",
    "  nₜ = tanh(Wₙ xₜ + Uₙ (rₜ ⊙ hₜ₋₁))  \n",
    "  hₜ = (1 - zₜ) ⊙ nₜ + zₜ ⊙ hₜ₋₁ + tanh(W_res xₜ)\n",
    "\n",
    "The Encoder uses a standard multi-layer GRU:\n",
    "\n",
    "  hₜ = GRU(xₜ, hₜ₋₁)\n",
    "\n",
    "The Decoder processes one token at a time using a stack of custom GRU cells.  \n",
    "The final output is generated as:\n",
    "\n",
    "  yₜ = W_out hₜ\n",
    "\n",
    "*Prepared by Ziming Liu*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJN24OtFqhnI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU4oPTOlqolw"
   },
   "source": [
    "\n",
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DES0LcMIqmmq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sample text from Shakespeare (demonstration corpus)\n",
    "text = (\n",
    "    \"To be, or not to be, that is the question: \"\n",
    "    \"Whether 'tis nobler in the mind to suffer \"\n",
    "    \"The slings and arrows of outrageous fortune, \"\n",
    "    \"Or to take arms against a sea of troubles, \"\n",
    "    \"And by opposing end them. To die: to sleep; \"\n",
    "    \"No more; and by a sleep to say we end \"\n",
    "    \"The heart-ache and the thousand natural shocks \"\n",
    "    \"That flesh is heir to, 'tis a consummation \"\n",
    "    \"Devoutly to be wish'd. To die, to sleep; \"\n",
    ")\n",
    "\n",
    "# Create a character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Convert the entire text into integer indices\n",
    "data = [char2idx[ch] for ch in text]\n",
    "\n",
    "# Set sequence length and create training examples (input, target pairs)\n",
    "seq_length = 40\n",
    "step = 1\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "\n",
    "##########################################\n",
    "# Your code starts here.\n",
    "\n",
    "# generate the sentense pairs of seq_length\n",
    "for i in range(0, len(data) - seq_length, step):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Your code ends here.\n",
    "##########################################\n",
    "print(f\"Number of sequences: {len(input_sequences)}\")\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_tensor = torch.tensor(input_sequences, dtype=torch.long)\n",
    "target_tensor = torch.tensor(target_sequences, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNTKYFTrquEB"
   },
   "source": [
    "# 2. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qeE7h2eBc_QQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Encoder: A multi-layer GRU encoder that processes the input sequence.\n",
    "# The GRU computes: h_t = GRU(x_t, h_{t-1})\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, dropout=0.2)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (seq_len, batch_size)\n",
    "        # Embedded input: x_t\n",
    "        embedded = self.embedding(src)\n",
    "        # GRU computes h_t from x_t and previous hidden state: h_t = GRU(x_t, h_{t-1})\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        # outputs: (seq_len, batch, hidden)\n",
    "        return outputs, hidden\n",
    "\n",
    "# Custom GRU Cell with a residual connection from the input.\n",
    "# Equations:\n",
    "#   r_t = σ(W_r x_t + U_r h_{t-1})\n",
    "#   z_t = σ(W_z x_t + U_z h_{t-1})\n",
    "#   n_t = tanh(W_n x_t + U_n (r_t ⊙ h_{t-1}))\n",
    "#   h_t = (1 - z_t) ⊙ n_t + z_t ⊙ h_{t-1} + tanh(W_res x_t)\n",
    "class CustomGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CustomGRUCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        ##########################################\n",
    "        # Your code starts here.\n",
    "\n",
    "        # Reset gate parameters with no bias\n",
    "        self.W_r =\n",
    "        self.U_r =\n",
    "\n",
    "        # Update gate parameters with no bias\n",
    "        self.W_z =\n",
    "        self.U_z =\n",
    "\n",
    "        # Candidate hidden state parameters\n",
    "        self.W_n =\n",
    "        self.U_n =\n",
    "\n",
    "        # Residual connection transformation\n",
    "        self.W_res = nn.Linear(input_size, hidden_size)\n",
    "        # Your code ends here.\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, x, h):\n",
    "\n",
    "        ##########################################\n",
    "        # Your code starts here.\n",
    "        # x: (batch, input_size), h: (batch, hidden_size)\n",
    "        # Compute reset gate: r_t = σ(W_r x_t + U_r h_{t-1})\n",
    "        r =\n",
    "        # Compute update gate: z_t = σ(W_z x_t + U_z h_{t-1})\n",
    "        z =\n",
    "        # Compute candidate hidden state: n_t = tanh(W_n x_t + U_n (r_t ⊙ h_{t-1}))\n",
    "        n =\n",
    "        # Residual connection: transform x and add to the new state\n",
    "        res =\n",
    "        # Compute new hidden state: h_t = (1 - z_t) ⊙ n_t + z_t ⊙ h_{t-1} + tanh(W_res x_t)\n",
    "        h_new =\n",
    "        # Your code ends here.\n",
    "        ##########################################\n",
    "        return h_new\n",
    "\n",
    "# Decoder: A multi-layer decoder that uses a stack of custom GRU cells.\n",
    "# For each layer, the custom GRU cell processes the input as defined above.\n",
    "# The final output is computed as: y_t = W_out h_t\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.num_layers = num_layers\n",
    "        ##########################################\n",
    "        # Your code starts here.\n",
    "        # Create custom GRU cells for each layer\n",
    "        self.custom_cells = nn.ModuleList([\n",
    "\n",
    "        ])\n",
    "        # Your code ends here.\n",
    "        ##########################################\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input: (batch) token indices; embed to (batch, embed_size)\n",
    "        embedded = self.embedding(input)\n",
    "        new_hidden = []\n",
    "        x = embedded\n",
    "        # Pass input through each custom GRU cell sequentially\n",
    "        for i, cell in enumerate(self.custom_cells):\n",
    "            h = hidden[i]\n",
    "            h_new = cell(x, h)\n",
    "            new_hidden.append(h_new)\n",
    "            x = h_new  # The output of one layer is the input to the next\n",
    "        new_hidden = torch.stack(new_hidden, dim=0)  # (num_layers, batch, hidden)\n",
    "        # Compute the final output token logits: y_t = W_out h_t\n",
    "        output = self.fc_out(x)\n",
    "        return output, new_hidden\n",
    "\n",
    "# Seq2Seq Model: Combines the Encoder and Decoder.\n",
    "# The model generates the output sequence token by token.\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src: (seq_len, batch), trg: (seq_len, batch)\n",
    "        batch_size = src.size(1)\n",
    "        trg_len = trg.size(0)\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, vocab_size).to(self.device)\n",
    "\n",
    "        # Encode the input sequence: h_t = GRU(x_t, h_{t-1})\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # The first input token to the decoder is the first token of the target sequence\n",
    "        input_dec = trg[0, :]\n",
    "\n",
    "        ##########################################\n",
    "        # Your code starts here.\n",
    "        # Generate the output sequence token by token\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden\n",
    "            outputs[t] = output\n",
    "            # Use the highest scoring token as the next input\n",
    "            top1 =\n",
    "            input_dec = top1\n",
    "        # Your code ends here.\n",
    "        ##########################################\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3xqqtcjtIAN"
   },
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9XA5DFVsPak"
   },
   "outputs": [],
   "source": [
    "# Takes a bit more than 1 min on Google Colab T4 GPU\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_epochs = 50\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "\n",
    "dataset_size = input_tensor.size(0)\n",
    "print(\"Dataset size:\", dataset_size)\n",
    "\n",
    "# Initialize the Encoder, Decoder, and the Seq2Seq model\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Function to generate random mini-batches from the training data\n",
    "def get_batch(batch_size):\n",
    "    indices = np.random.choice(dataset_size, batch_size)\n",
    "    src = input_tensor[indices].t().to(device)   # (seq_len, batch)\n",
    "    trg = target_tensor[indices].t().to(device)    # (seq_len, batch)\n",
    "    return src, trg\n",
    "\n",
    "\n",
    "# The training loop applies gradient clipping to stabilize training.\n",
    "all_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = max(1, dataset_size // batch_size)\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        src, trg = get_batch(batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        # Reshape output for loss computation\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].reshape(-1, output_dim)  # Ignore the first token\n",
    "        trg_loss = trg[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg_loss)\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    all_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot the training loss curve over epochs\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(all_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK76hvgJtPHd"
   },
   "source": [
    "# 4. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onqgi2-YsQJr"
   },
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 5. Text Generation\n",
    "# --------------------------\n",
    "# This function generates text using the trained model starting from a given string.\n",
    "def generate_text(model, start_str, gen_length=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_seq = [char2idx[ch] for ch in start_str]\n",
    "    input_tensor_gen = torch.tensor(input_seq, dtype=torch.long).unsqueeze(1).to(device)  # (seq_len, 1)\n",
    "\n",
    "    # Encode the input sequence\n",
    "    encoder_outputs, hidden = encoder(input_tensor_gen)\n",
    "\n",
    "    # Use the last character of the input as the first token for the decoder\n",
    "    input_dec = input_tensor_gen[-1, :]\n",
    "    generated = start_str\n",
    "\n",
    "    for _ in range(gen_length):\n",
    "        output, hidden = decoder(input_dec, hidden)\n",
    "        # Apply temperature sampling to control randomness in generation\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        char = idx2char[top_i.item()]\n",
    "        generated += char\n",
    "        input_dec = torch.tensor([top_i.item()]).to(device)\n",
    "\n",
    "    return generated\n",
    "\n",
    "# Generate and display sample text from the trained model\n",
    "start_string = \"To be\"\n",
    "generated_text = generate_text(model, start_string, gen_length=300, temperature=0.8)\n",
    "print(\"\\nGenerated Text:\\n\")\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
